{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADASS2019_part_A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almicol/SSAPServer/blob/master/ADASS2019_part_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQzOXYo_a3T_",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "\n",
        "<img src=\"https://docs.google.com/uc?export=download&id=1qObOQ2sNwn9_PvMbr_ztrg_0jvKltwip\" alt=\"introduction to artificial neural networks\">\n",
        "\n",
        "![https://creativecommons.org/licenses/by-nc-sa/4.0/](https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png \"https://creativecommons.org/licenses/by-nc-sa/4.0/\")\n",
        "</center>\n",
        "\n",
        "<font size=1>\n",
        "presented by Kai Polsterer and Nikos Gianniotis, HITS gGmbH at ADASS XXIX in Groningen, the Netherlands. Not taking responsibility for anything including proper functionality and content of links. Not to be tweeted about by Kirk Borne or science kardashians! https://en.wikipedia.org/wiki/Kardashian_Index\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K28gGLPb2tu",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "#Recap of last ADASS\n",
        "\n",
        "<font size=4>\n",
        "\n",
        "During the last ADASS, in the **Beginners Guide to Machine Learning**, we heard about the basics of machine learning. Key topics we spoke about included:\n",
        "\n",
        "* classification with reference data / how to learn from data\n",
        "* evaluation of performance / what does performance mean\n",
        "* validation of results / does the model generalize well\n",
        "* unbalanced data sets / what does 99% correct mean when 99% are of class A\n",
        "* regression with reference data / predicting values instead of class labels\n",
        "* nearest neighbours, decision trees / partitioning the feature space\n",
        "* ensemble methods, random forest / combining weak learners\n",
        "* feature selection / an example of scientific use\n",
        "* **neural networks** / we just started our journey\n",
        "\n",
        "We learned that machine learning is **the quest of finding a good predictive model**. Today we want to understand artificial neural networks to be able to write our own models from scratch. In the end, you will understand that artificial inteligence is currently a favoured approach for training models, but that a dystropic future with killing robots is still far away.<br><br>\n",
        "\n",
        "\n",
        "<font size=2><center>\n",
        "<table><tr><td>related information</td></tr>\n",
        "<tr><td>\n",
        "\n",
        "[adass2018.astro.umd.edu/bof.html](http://adass2018.astro.umd.edu/bof.html)\n",
        "\n",
        "[tinyurl.com/bg2mlBoF](https://tinyurl.com/bg2mlBoF)\n",
        "\n",
        "</td></tr></table>\n",
        "</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of5thwQwYiQO",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Overview\n",
        "\n",
        "<font size=4>\n",
        "\n",
        "This tutorial is an extension to the activities of previous ADASS meetings, which aimed for providing an overview of different aspects of machine learning. \n",
        "This is a hands on introduction to neural networks.  \n",
        "The participant will be exposed to the basic principles underlying artificial neural networks. To do so, we will start from very simple models, show relevant equations and how to code these equations in ``python``.\n",
        "We will attempt to provide an overview of key concepts in order to facilitate the interested paricipants in their self study. <br><br>\n",
        "\n",
        "  \n",
        "\n",
        "This tutorial is presented in an interactive format. Similar to ”Beginners guide to machine learning”\n",
        "a ``jupyter`` notebook is provided that participants can use during or after the session to reproduce any subset of the presented material. <br><br>\n",
        "  \n",
        "  \n",
        "<font size=5>\n",
        "\n",
        "- [Part A](https://colab.research.google.com/drive/1oZC3NMMnphThodpyw0GdLMVqFIzrtpHr): **from simple lines to neurons**\n",
        "- [Part B](https://colab.research.google.com/drive/1hIaFKr5At02wUaFHNQpDn0HuhaIE-F6s): from simple neurons to more complex architectures\n",
        "- [Part C](https://colab.research.google.com/drive/1MwgeXxaezdpUYbgEMi00oNzch7Paz7Ou): from complex architectures to deep convolutional networks\n",
        "- [Part D](https://colab.research.google.com/drive/1GmSDz9SKT82ZFGKL6DNbCWPCdMNSZN6E): summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pof2YOtYWxV",
        "colab_type": "text"
      },
      "source": [
        "### Let us setup various things that we will need for this sesssion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrBl-nG8YFQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# whatever general tools are necessary; \n",
        "# specific things are introduces in the relevant cells\n",
        "import numpy\n",
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "numpy.random.seed(12347) # initialize random number generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTv_RWIOExpS",
        "colab_type": "text"
      },
      "source": [
        "#From simple lines to neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srPGtwpmY9DX",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##Fitting a straight line\n",
        "\n",
        "<font size=4>\n",
        "We start with the most basic building block.\n",
        "Given pairs of data items <font color=#bb0000>$(x\\in R, y \\in R)$</font>, we want to fit a line. This means, to find the line (of all possible lines) that is the closest to the given  data.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heTYNQsStViu",
        "colab_type": "text"
      },
      "source": [
        "###Defining a model\n",
        "<font size=4>\n",
        "A line is defined as:<br>\n",
        "<font color=#bb0000>$y = \\alpha x + \\beta$</font>\n",
        "\n",
        "We use  notation that is more typical in the  machine learning setting:<br>\n",
        "\n",
        "<font color=#bb0000>$f(x;w_0,w_1) = w_1 x + w_0$</font><br><br>\n",
        "\n",
        "The linear model translates into the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oyXrckwZLB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_model(x, w0, w1):\n",
        "  return x*w1 + w0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGiwUoZWGT2X",
        "colab_type": "text"
      },
      "source": [
        "###Generating synthetic data\n",
        "<font size=4>\n",
        "Let's randomly generate some data for <font color=#bb0000>$x$</font> and <font color=#bb0000>$y$</font> using our linear model and plot them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi9Agp53gr58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.3\n",
        "beta = 0.5\n",
        "\n",
        "x = numpy.random.uniform(size = 10) # generate 10 values between 0 and 1\n",
        "y = linear_model(x, beta, alpha)\n",
        "\n",
        "xvals = numpy.arange(0,1,1/10)\n",
        "pyplot.figure(figsize=(16,6))\n",
        "pyplot.plot(xvals, linear_model(xvals, beta, alpha), c='g', lw=0.4)\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i3oCI9z1nrG",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>\n",
        "Typically measured data are noisy, so we simulate this by adding some noise to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVGYsN2ez9JG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = y + numpy.random.normal(loc=0.0, scale=0.01, size=x.shape[0]) # add some additional noise with a sigma of 0.01\n",
        "\n",
        "pyplot.figure(figsize=(16,6))\n",
        "pyplot.plot(xvals, linear_model(xvals, beta, alpha), c='g', lw=0.4)\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whV_TuGBZkcn",
        "colab_type": "text"
      },
      "source": [
        "###Defining an error function\n",
        "<font size=4>\n",
        "Our model is a function that describes a line.\n",
        "Fitting a line means, finding the <font color=#bb0000>$w_0$</font> and <font color=#bb0000>$w_1$</font> parameters that give us the <font color=#bb0000>$f(x;w_0,w_1)$</font> closest to the observed data.<br><br>\n",
        "\n",
        "But what does closest mean? Let's define closeness as the error function:\n",
        "\n",
        "<font color=#bb0000>$e(w_0,w_1) = \\sum_{n=1}^N \\|f(x_n;w_0,w_1) - y_n\\|^2$</font><br><br>\n",
        "\n",
        "This translates directly into:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS2HtIRSaykb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implements squared-error objective e(w0,w1) to be minimised\n",
        "def error_of_linear_model(x, y, param):\n",
        "\n",
        "  # unpacking of parameters\n",
        "  w0 = param[0]\n",
        "  w1 = param[1]\n",
        "\n",
        "  # there are N data items\n",
        "  N = x.shape[0]\n",
        "\n",
        "  # get predictions from model\n",
        "  predictions = numpy.zeros((N))\n",
        "  for n in range(N):\n",
        "    predictions[n] = linear_model(x[n], w0, w1)\n",
        "\n",
        "  # calculate discrepancy\n",
        "  error = 0.0\n",
        "  for n in range(N):\n",
        "    error = error + (predictions[n] - y[n])**2\n",
        "\n",
        "  # return error to be minimised\n",
        "  return error\n",
        "\n",
        "# compact alternative to the method above\n",
        "# def error_of_linear_model(x, y, param):\n",
        "#   return numpy.sum(numpy.square(linear_model(x, param[0], param[1])-y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7aRYbQQ29im",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>We can use the error function to easily determine which parameter values produce a line closer to the data, i.e. that fits the data better. The smaller the error, the better the fit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvJUZ49CkkMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (error_of_linear_model(x, y, [0.55, 0.35]))\n",
        "print (error_of_linear_model(x, y, [beta, alpha]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKq4WscXZnev",
        "colab_type": "text"
      },
      "source": [
        "###Minimizing the objective function by using the gradient\n",
        "<font size=4>\n",
        "The introduced error function is the objective function we want to minimise with respect to parameters <font color=#bb0000>$w_0$</font> and <font color=#bb0000>$w_1$</font>. This is actually a least-squares problem and has a closed form solution.<br><br>\n",
        "\n",
        "However: in anticipation of more complex models that we will introduce later, we solve the problem using gradient based optimisation.\n",
        "\n",
        "Gradient based optimisation improves the model fit incrementally:\n",
        "* It calculates the gradient <font color=#bb0000>$\\nabla e(w_0, w_1)$</font> of the objective function wrt <font color=#bb0000>$w_0$</font> and <font color=#bb0000>$w_1$</font>.\n",
        "\n",
        "* The calculated gradient tells us how to update the current values of parameters\n",
        "<font color=#bb0000>$w_0$</font> and <font color=#bb0000>$w_1$</font> in order to incrementally improve the model fit.\n",
        "  \n",
        "* Repeatedly updating the parameters values, should lead us to the minimum of the objective function <font color=#bb0000>$e(w_0, w_1)$</font>.\n",
        "\n",
        "This can be intuitively illustrated by the following figure.</font>\n",
        "\n",
        "<figure><center>\n",
        "    <img src=\"https://docs.google.com/uc?export=download&id=1SScx3PXbwqh5dsevGMYeVU_ttSKThjTi\" alt=\"Gradient Descent\"></center>\n",
        "    <figcaption><b>Figure 1</b>: The figure shows the 2D plot of the objective function $e(w_0, w_1)$ which we wish to minimise. The red arrows show the direction of the <b>negative</b> gradient. Gradient points towards higher values  of the objective function, while <b>negative</b> gradient points towards lower values. Following the <b>negative</b> gradient leads us to the minimum of $e(w_0, w_1)$ and best solution for $w_0, w_1$.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAecx8_Ysed4",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>The gradient of <font color=#bb0000>$e(w_0, w_1)$</font> wrt <font color=#bb0000>$w_0, w_1$</font> reads:\n",
        "\n",
        "<font color=#bb0000>$\\nabla e(w_0, w_1) = \\begin{bmatrix} 2(f(x_n;w_0,w_1) -y_n) \\\\ 2(f(x_n;w_0,w_1) -y_n)x_n \\end{bmatrix}$</font> <br><br>\n",
        "<font size=2><center>\n",
        "<table><tr><td>related information</td></tr>\n",
        "<tr><td>\n",
        "\n",
        "[https://en.wikipedia.org/wiki/Gradient](https://en.wikipedia.org/wiki/Gradient)\n",
        "\n",
        "[https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
        "\n",
        "[https://www.wolframalpha.com/](https://www.wolframalpha.com/widgets/view.jsp?id=5d20575fa6cbe9d1e5a046aeef36e7a3)\n",
        "\n",
        "</td></tr></table>\n",
        "</center></font>\n",
        "\n",
        "We code the gradient as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArrzJFaQZt-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implements gradient of squared-error objective\n",
        "def calculate_gradient_for_linear_model(x, y, param):\n",
        "\n",
        "  # unpacking of parameters\n",
        "  w0 = param[0]\n",
        "  w1 = param[1]\n",
        "\n",
        "  # there are N data items\n",
        "  N = x.shape[0]\n",
        "\n",
        "  # calculate gradient\n",
        "  gradient = numpy.zeros((2))\n",
        "  for n in range(N):\n",
        "    prediction  = linear_model(x[n], w0, w1)\n",
        "    gradient[0] = gradient[0] + 2.0*(prediction - y[n])*1.0\n",
        "    gradient[1] = gradient[1] + 2.0*(prediction - y[n])*x[n]\n",
        "\n",
        "  # return gradient\n",
        "  return gradient\n",
        "\n",
        "# compact alternative to the method above\n",
        "# def calculate_gradient_for_linear_model(x, y, param):\n",
        "#   return numpy.sum(2*(linear_model(x, param[0], param[1])-y)), numpy.sum(2*(linear_model(x, param[0], param[1])-y)*x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_saj2RSTwaRl",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>This allows us to calculate the gradient for a given set of parameters and plot the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de9jZ4iNl5To",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "errors = numpy.array([error_of_linear_model(x,y,par) for par in numpy.mgrid[-1:1:101j, -1:1:101j].T.reshape(-1,2)]).reshape(101,101).T\n",
        "\n",
        "def plot_data_and_gradient(x, y, w0, w1, eta):\n",
        "  pyplot.subplot2grid((1,2),(0,0))\n",
        "  pyplot.plot(xvals, linear_model(xvals, beta, alpha), c='g', lw=0.4) # true line\n",
        "  pyplot.plot(xvals, linear_model(xvals, w0, w1), c='r', lw=0.6)\n",
        "  pyplot.scatter(x,y)\n",
        "  pyplot.xlabel(\"$x$\")\n",
        "  pyplot.ylabel(\"$y$\")\n",
        "  pyplot.subplot2grid((1,2),(0,1))\n",
        "  pyplot.xlim(-1,1)\n",
        "  pyplot.xlabel(\"$w_0$\")\n",
        "  pyplot.ylim(-1,1)\n",
        "  pyplot.ylabel(\"$w_1$\")\n",
        "  pyplot.scatter(beta, alpha, c='g') # true parameters\n",
        "  pyplot.imshow(numpy.flipud(errors.T), extent=[-1,1,-1,1], alpha=0.5, cmap='gray_r', aspect='auto')\n",
        "  pyplot.contour(numpy.arange(-1,1,2/101),numpy.arange(-1,1,2/101),errors.T, levels=[0.1,0.5,2,5,10,20], alpha=0.5)\n",
        "  pyplot.scatter(w0, w1, c='r')\n",
        "  gradient = calculate_gradient_for_linear_model(x,y,[w0, w1])\n",
        "  pyplot.arrow(w0, w1, -gradient[0]*eta, -gradient[1]*eta, head_width=0.05, alpha=0.3)\n",
        "\n",
        "pyplot.figure(figsize=(16,6))\n",
        "plot_data_and_gradient(x, y, -0.15, -0.65, eta=0.01)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ1r2alUkFXW",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>Now that we have the gradient, we can do gradient based optimisation. Just like we\n",
        "explained in the figure, we follow the **negative** gradient by taking small steps in a number of iterations.\n",
        "\n",
        "We start from a random guess for <font color=#bb0000>$w_0, w_1$</font>.\n",
        "\n",
        "Iteratively:\n",
        "* calculate the gradient\n",
        "* update the parameters by taking a small step towards the **negative** gradient\n",
        "* repeat until the objective does not improve significantly\n",
        "\n",
        "<font color=#bb0000>$w_0 \\leftarrow w_0 - \\eta \\nabla e(w_0, w_1)_1$</font> # take 1-st component of gradient \n",
        "\n",
        "<font color=#bb0000>$w_1 \\leftarrow w_1 - \\eta \\nabla e(w_0, w_1)_2$</font> # take 2-nd component of gradient \n",
        "\n",
        "\n",
        "For a number of iterations, e.g. 1000, we calculate the gradient and update the parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDpWCd8mkH_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "# initialise parameters randomly\n",
        "w0 = numpy.random.normal()\n",
        "w1 = numpy.random.normal()\n",
        "print (w0, w1)\n",
        "\n",
        "# step size (also known as learning rate)\n",
        "eta = 0.01\n",
        "\n",
        "for iteration in range(500):\n",
        "    # calculate gradient\n",
        "    gr = calculate_gradient_for_linear_model(x, y, [w0,w1])\n",
        "\n",
        "    # take a step towards the *negative* gradient\n",
        "    w0 = w0 - eta*gr[0]\n",
        "    w1 = w1 - eta*gr[1]\n",
        "\n",
        "    if iteration%10==0:\n",
        "      print(\"Iteration %d error is %f\" % (iteration, error_of_linear_model(x,y,[w0,w1])))\n",
        "      clear_output(wait=True)\n",
        "      pyplot.figure(figsize=(16,6))\n",
        "      plot_data_and_gradient(x, y, w0, w1, eta)\n",
        "      pyplot.show()\n",
        "\n",
        "print (w0, w1) # final result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyFxTYETrCLx",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Using automatic differentiation (AD) and optimisation routines\n",
        "<font size=4>\n",
        "In the example of fitting a line, we implemented our own optimisation routine:\n",
        "\n",
        "* we calculated the gradient by hand\n",
        "\n",
        "* we implemented a simple optimiser called gradient descent\n",
        "\n",
        "While this is useful for understanding what is involved in fitting a model,\n",
        "in everyday work we want use libraries to do this work for us:\n",
        "\n",
        "* AD calculates the gradient for us\n",
        "\n",
        "* optimisation libraries offer sophisticated and numerically robust optimisers\n",
        "\n",
        "\n",
        "The package `autograd` offers functionality for calculating gradients automatically (other packages are available). We can now do this for our squared error function wrt to <font color=#bb0000>$w_0, w_1$</font>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovzmV-bIr-9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import autograd.numpy as np  # Thinly-wrapped numpy\n",
        "from autograd import grad    # the only autograd function you may ever need\n",
        "\n",
        "def compact_error_of_linear_model(param): # note that we omit x and y, as they are not changing parameters of the function\n",
        "  return numpy.sum(numpy.square(linear_model(x, param[0], param[1])-y))\n",
        "  \n",
        "grad_f = grad(compact_error_of_linear_model)  # obtain its gradient function\n",
        "\n",
        "print (grad_f([0.6,0.5]))\n",
        "print (calculate_gradient_for_linear_model(x,y,[0.6, 0.5]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mc1M-FLJn5a",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>Let us calculate the gradient for the somewhat more complex example function <font color=#bb0000>$f(x,y)=((x+1)^2 + 2(y-1)^2)$</font> with respect to <font color=#bb0000>$x,y$</font>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fX2OfOYAKSZo",
        "colab": {}
      },
      "source": [
        "# Example adapted from https://github.com/HIPS/autograd\n",
        "\n",
        "def f(p):                    # Define a function\n",
        "  return (p[0]+1)**2 + 2*(p[1]-1)**2\n",
        "\n",
        "grad_f = grad(f)             # Obtain its gradient function\n",
        "grad_f([3.0,4.0])            # result should be [8; 12]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ht7CCJsQxg",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>The minimum value of our example function <font color=#bb0000>$f$</font> is <font color=#bb0000>$0$</font> attained at <font color=#bb0000>$(x,y)=(-1,1)$</font>\n",
        "\n",
        "Let us find the minimum using `scipy.optimize.minimize` (other packages are available).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3ytfmPZtC4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# first argument is the function we want to minimise\n",
        "# second argument is the starting point - here we start from a random pair of values\n",
        "# the third argument is the optimiser - here we use conjugate gradient, a sophisticated method\n",
        "# the fourth argument provides the gradient function, which we compute with automatic differentiation\n",
        "\n",
        "minimize(f,np.random.rand(2),method='CG',jac=grad_f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAL9oOV2xCLa",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>In the next models we present, instead of doing things manually, we put these tools at work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frKLvN4xKIoE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Fitting a plane\n",
        "<font size=4>\n",
        "\n",
        "We saw how to fit a line for data where the input was <font color=#bb0000>$x\\in R$</font>.\n",
        "What if the input has more dimensions?\n",
        "In this example, we work with data pairs <font color=#bb0000>$(x\\in R^2, y \\in R)$</font> where the input <font color=#bb0000>$x$</font> has two dimensions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1gYQgOfAk0s3"
      },
      "source": [
        "###Defining a model\n",
        "<font size=4>\n",
        "\n",
        "A plane is defined through:<br>\n",
        "\n",
        "<font color=#bb0000>$f(\\mathbf{x}) = w_2 x_2 + w_1x_1 + w_0$</font>\n",
        "\n",
        "Let us use a more convenient notation that is common in machine learning:\n",
        "\n",
        "<font color=#bb0000>$f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x},\\quad where\\ \\mathbf{x}=\\begin{bmatrix}\n",
        "1\\\\\n",
        "x_1\\\\\n",
        "x_2\n",
        "\\end{bmatrix} \\ and \\ \\mathbf{w}=\\begin{bmatrix}\n",
        "w_0\\\\\n",
        "w_1\\\\\n",
        "w_2\n",
        "\\end{bmatrix}$</font>\n",
        "\n",
        "The plane model translates into the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmd5B5hZjvEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plane_model(x, w):\n",
        "  return numpy.dot(w.T, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C09ahDFFkCvU",
        "colab_type": "text"
      },
      "source": [
        "<font size=2><center>\n",
        "<table><tr><td>related information</td></tr>\n",
        "<tr><td>\n",
        "\n",
        "[https://en.wikipedia.org/wiki/Dot_product](https://en.wikipedia.org/wiki/Dot_product)\n",
        "\n",
        "[https://en.wikipedia.org/wiki/Transpose](https://en.wikipedia.org/wiki/Transpose)\n",
        "\n",
        "</td></tr></table>\n",
        "</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tWsbMIfDndIJ"
      },
      "source": [
        "###Generating synthetic data\n",
        "<font size=4>\n",
        "Let's generate some random data for <font color=#bb0000>$\\mathbf{x}$</font> and <font color=#bb0000>$y$</font> using our plane model. Accordingly, we have to augment each input <font color=#bb0000>$\\mathbf{x}_n$</font> with element <font color=#bb0000>$1.0$</font>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HXUz879animK",
        "colab": {}
      },
      "source": [
        "n = 100\n",
        "w = numpy.array([4.3, 2.5, 1.2])\n",
        "x = numpy.random.uniform(size = (2,n)) # generate n 2d values between 0 and 1\n",
        "x = numpy.concatenate((numpy.ones((1,n)), x), axis=0) # augument with 1\n",
        "y = plane_model(x, w) + numpy.random.normal(loc=0.0, scale=0.1, size=x.shape[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwP6aWoIjznF",
        "colab_type": "text"
      },
      "source": [
        "###Defining the error function and minimizing it with AD and gradient descent\n",
        "<font size=4>How about the objective function to be minimised? Looks almost the same to the one used for fitting a line:\n",
        "\n",
        "<font color=#bb0000>$e(\\mathbf{w}) = \\sum_{n=1}^N \\|f(\\mathbf{x}_n;\\mathbf{w}) - y_n\\|^2$</font>\n",
        "\n",
        "The code for the objective function is **very** similar to the one for fitting a straight line:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpUi6ftVj2bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def error_of_plane_model(x, y, w):\n",
        "  return numpy.sum(numpy.square(plane_model(x, w)-y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwxMd-l_j5_t",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>We calculate the gradient using AD:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpfuF2Bej9Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use an anonymous function to convey that objective function\n",
        "# to be optimised is solely a function of the free parameters w\n",
        "error_of_plane = lambda w: error_of_plane_model(x, y, w)\n",
        "grad_objective = grad(error_of_plane)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w9YIuXMkBRj",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>Now we call the optimiser:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVGVJWH1kDVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = minimize(error_of_plane, numpy.random.rand(3), method='CG', jac=grad_objective)\n",
        "print (result.x, w, sep='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nvqbBRg8AOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "figure = pyplot.figure(figsize=(16,10))\n",
        "ax = figure.add_subplot(111, projection='3d')\n",
        "ax.scatter(x.T[:,1], x.T[:,2], y)\n",
        "\n",
        "xx, yy = numpy.meshgrid(range(11), range(11))\n",
        "z = numpy.array([plane_model([1,par[0],par[1]],w) for par in numpy.mgrid[0:1:11j, 0:1:11j].T.reshape(-1,2)]).reshape(11,11).T\n",
        "ax.plot_surface(xx/10, yy/10, z, alpha=0.2)\n",
        "\n",
        "xx, yy = numpy.meshgrid(range(11), range(11))\n",
        "z = numpy.array([plane_model([1,par[0],par[1]],result.x) for par in numpy.mgrid[0:1:11j, 0:1:11j].T.reshape(-1,2)]).reshape(11,11).T\n",
        "ax.plot_surface(xx/10, yy/10, z, alpha=0.2, color='r')\n",
        "#ax.view_init(20, 123)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0tATmxHdwtY",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Fitting a polynomial\n",
        "<font size=4>\n",
        "We know now how to fit lines and (hyper-) planes. What if our problem is non-linear? Instead of a straight line, we can fit a curve, for instance a polynomial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SnxiuW7x5gSt"
      },
      "source": [
        "###Defining a model\n",
        "<font size=4>\n",
        "The polynomial model is given by:\n",
        "\n",
        "<font color=#bb0000>$f(x) = w_3 x^3 + w_2 x^2 + w_1 x + w_0$</font>\n",
        "\n",
        "Or equivalently, using our convenient notation as before:\n",
        "\n",
        "<font color=#bb0000>$f(x) = \\mathbf{w}^T h(x),\\quad where\\ h(x)=\\begin{bmatrix}\n",
        "1\\\\\n",
        "x\\\\\n",
        "x^2\\\\\n",
        "x^3\n",
        "\\end{bmatrix} \\ and \\ \\mathbf{w}=\\begin{bmatrix}\n",
        "w_0\\\\\n",
        "w_1\\\\\n",
        "w_2\\\\\n",
        "w_3\n",
        "\\end{bmatrix}$\n",
        "</font>\n",
        "\n",
        "The vector <font color=#bb0000>$h(x)$</font> is called a basis function. Here we use a polynomial basis. \n",
        "\n",
        "The basis model as well as the polynomial model is expressed in ``python`` as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz6dk_r06ZJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def h(x):\n",
        "  return numpy.array([numpy.ones(x.shape), x, x*x, x*x*x])\n",
        "\n",
        "def polynomial_model(x, w):\n",
        "  return numpy.dot(w.T, h(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53pJv5jVAe1U",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>Notice that the expression and code read the same as for line and plane examples.\n",
        "This is **no** coincidence.\n",
        "\n",
        "The polynomial model, although it is solving a non-linear regression task,\n",
        "is still linear in the sense that the parameters interact linearly with the data, the basis <font color=#bb0000>${h}(x)$</font>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OaFaBhmZ4zIH"
      },
      "source": [
        "###Generating synthetic data\n",
        "<font size=4>\n",
        "Let's randomly generate some data of pairs <font color=#bb0000>$(x\\in R,y\\in R)$</font> using our polynomial model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0RRW2mZ8OQ_",
        "colab": {}
      },
      "source": [
        "n = 10\n",
        "\n",
        "w = numpy.array([3.1, 0.5, 1.2, -3])\n",
        "x = numpy.random.uniform(size = (n)) # generate n values between 0 and 1\n",
        "y = polynomial_model(x, w) + numpy.random.normal(loc=0.0, scale=0.04, size=x.shape[-1]) # add some noise\n",
        "\n",
        "xvals = numpy.arange (-0.7,1.1,0.1)\n",
        "pyplot.figure(figsize=(16,6))\n",
        "pyplot.plot(xvals, polynomial_model(xvals, w), c='g', lw=0.4)\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDnlXK1gA5Ff",
        "colab_type": "text"
      },
      "source": [
        "###Defining the error function and minimizing it with AD and gradient based optimisation\n",
        "<font size=4>How about the objective function to be minimised? It's the **same** as for fitting a plane; the squared error:\n",
        "\n",
        "<font color=#bb0000>$e(\\mathbf{w}) = \\sum_{n=1}^N \\|f(\\mathbf{x}_n;\\mathbf{w}) - y_n\\|^2$</font>\n",
        "\n",
        "The code is very similar and reads:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Ts4-xKMB_ZH",
        "colab": {}
      },
      "source": [
        "def error_of_polynomial_model(x, y, w):\n",
        "  return numpy.sum(numpy.square(polynomial_model(x, w)-y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcfO6P0CiVB",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>We now can calculate the gradient, use the optimizer and visualize the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pThFgJbwCxR4",
        "colab": {}
      },
      "source": [
        "error_of_polynomial = lambda w: error_of_polynomial_model(x, y, w)\n",
        "grad_objective = grad(error_of_polynomial)\n",
        "result = minimize(error_of_polynomial, numpy.random.rand(4), method='CG', jac=grad_objective)\n",
        "print (result.x, w, sep='\\n')\n",
        "\n",
        "pyplot.figure(figsize=(16,6))\n",
        "pyplot.plot(xvals, polynomial_model(xvals, w), c='g', lw=0.4)\n",
        "pyplot.plot(xvals, polynomial_model(xvals, result.x), c='r', lw=0.5)\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bYlR1GQR2YC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##Adaptive basis\n",
        "<font size=4>\n",
        "In the previous example we used a fixed basis (polynomial basis) <font color=#bb0000>$h(x) = [1, x, x^2, x^3]$</font>. A specific basis might be good for a particular problem. But how can we adapt the basis to the problem at hand?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XBDtI4OTUHe6"
      },
      "source": [
        "###Defining a model\n",
        "<font size=4>\n",
        "\n",
        "We can achieve this by making <font color=#bb0000>$h$</font> a parametric (i.e. adaptable) function <font color=#bb0000>$h(\\mathbf{x}; \\mathbf{v}, \\mathbf{b})$</font> with parameters <font color=#bb0000>$\\mathbf{v}$</font> and <font color=#bb0000>$\\mathbf{b}$</font>.<br><br>\n",
        "\n",
        "Basis function <font color=#bb0000>$h$</font> calculates the vector:\n",
        "\n",
        "<font color=#bb0000>$h(\\mathbf{x}; \\mathbf{v}, \\mathbf{b}) = \\begin{bmatrix} 1, \\tanh( \\mathbf{v}_1^T \\mathbf{x} + \\mathbf{b}_1), \\dots, \\tanh( \\mathbf{v}_M^T \\mathbf{x} + \\mathbf{b}_M) \\end{bmatrix}$</font>,\n",
        "\n",
        "where <font color=#bb0000>$\\mathbf{v}_m$</font> is the <font color=#bb0000>$m$</font>-th row of matrix <font color=#bb0000>$\\mathbf{v}$</font> and <font color=#bb0000>$\\mathbf{b}_m$</font> is the <font color=#bb0000>$m$</font>-th element of <font color=#bb0000>$\\mathbf{b}$</font>.\n",
        "This can be compactly written as:\n",
        "\n",
        "<font color=#bb0000>$h(\\mathbf{x}; \\mathbf{v},\\mathbf{b}) = \\begin{bmatrix} 1, \\tanh.(\\mathbf{v}^T \\mathbf{x} + \\mathbf{b})\\end{bmatrix}$</font>,\n",
        "\n",
        "if we use the convention that <font color=#bb0000>$\\tanh.(\\cdot)$</font> is the elementwise application of the ``tanh`` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_MC-BCPC7lC",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>\n",
        "We now consider the dimensions of the parameters. If the input is <font color=#bb0000>$\\mathbf{x}\\in R^{D}$</font>, then we need <font color=#bb0000>$\\mathbf{v}\\in R^{M\\times D}$</font>, i.e. a matrix, so that <font color=#bb0000>$\\mathbf{v}^T \\mathbf{x} \\in R^M$</font>. Consequently (in order to make the dimensions match), we want <font color=#bb0000>$\\mathbf{b}\\in R^{M}$</font>. <font color=#bb0000>$M$</font> is a constant of our choice whose meaning will become apparent later on. For the moment we arbitrarily set <font color=#bb0000>$M=3$</font> and we will see how to interpret this value soon.\n",
        "\n",
        "\n",
        "We code the adaptive basis as:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1PNDQSkqrVbH",
        "colab": {}
      },
      "source": [
        "def h(x, v, b):\n",
        "  return np.concatenate((np.ones((1,x.shape[1])),\n",
        "                            np.tanh(np.dot(v.T, x) + b)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaEqEsJwrZgF",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>\n",
        "We now contrast the parametric basis function <font color=#bb0000>$h$</font> to the polynomial calculated earlier:\n",
        "\n",
        "adaptive <font color=#bb0000>$h(\\mathbf{x}; \\mathbf{v}, \\mathbf{b}) =\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "\\tanh( \\mathbf{v}_1^T \\mathbf{x} + b_1) \\\\\n",
        "\\tanh( \\mathbf{v}_2^T \\mathbf{x} + b_2) \\\\\n",
        "\\tanh( \\mathbf{v}_3^T \\mathbf{x} + b_3) \\\\\n",
        "\\end{bmatrix}$</font>\n",
        "and polynomial\n",
        "<font color=#bb0000>$h(x) = \\begin{bmatrix} 1 \\\\ x \\\\x^2 \\\\ x^3 \\end{bmatrix}$</font>\n",
        "\n",
        "Hence, we can understand <font color=#bb0000>$h(\\mathbf{x}; \\mathbf{v}, \\mathbf{b})$</font> as calculating a vector of elements that are nonlinear versions of input <font color=#bb0000>$\\mathbf{x}_n$</font>.<br><br>\n",
        "\n",
        "\n",
        "The adaptive model reads\n",
        "<font color=#bb0000>$f(\\mathbf{x}) = \\mathbf{w}^T h(\\mathbf{x}; \\mathbf{v}, \\mathbf{b})$</font>. Note that the dimension of <font color=#bb0000>$\\mathbf{w}$</font> is <font color=#bb0000>$M+1$</font>.<br><br>\n",
        "\n",
        "The code for the adaptive model reads:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9FhUicnrilu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adaptive_model(x, v, b, w):\n",
        "  return np.dot(w.T, h(x, v, b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LiDGXANgo_TR"
      },
      "source": [
        "###Generating synthetic data\n",
        "<font size=4>\n",
        "Let's generate some random data for <font color=#bb0000>$x\\in R$</font> and <font color=#bb0000>$y\\in R$</font> by using the normalized sinus cardinalis <font color=#bb0000>$sinc(x) = \\frac{sin(\\pi x)}{\\pi x}$</font> as an example function that we would like to estimate from observed data pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTzM4trYqbVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 10\n",
        "x = numpy.random.uniform(low=-3, high=3, size=n).reshape(1,-1)\n",
        "y = numpy.sinc(x) + numpy.random.normal(scale=0.01, size=x.shape)\n",
        "\n",
        "xvals = numpy.arange (-3,3,0.1)\n",
        "pyplot.figure(figsize=(16,6))\n",
        "pyplot.plot(xvals, numpy.sinc(xvals), c='g', lw=0.5)\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X9WkPhA3KWyn"
      },
      "source": [
        "###Defining the error function and minimizing it with AD and gradient descent\n",
        "<font size=4>How about the objective function to be minimised? It's very similar to the squared error functions used above:\n",
        "\n",
        "<font color=#bb0000>$e(\\mathbf{v},\\mathbf{w},\\mathbf{b}) = \\sum_{n=1}^N \\|f(\\mathbf{x}_n;\\mathbf{v},\\mathbf{w},\\mathbf{b}) - y_n\\|^2$</font>\n",
        "\n",
        "In this experiment <font color=#bb0000>$x\\in R^1$</font> therefore  <font color=#bb0000>$D=1$</font> and we defined <font color=#bb0000>$M=3$</font>.\n",
        "To deal with all the parameters, a helper function to unpack <font color=#bb0000>$\\mathbf{v},\\mathbf{w},\\mathbf{b}$</font> is declared. The code of the error function is very similar and reads:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvybi9Xa0jUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = 1\n",
        "M = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-S6qHqP0CTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_parameter(param):\n",
        "  v = param[:D*M].reshape(M, D).T\n",
        "  b = param[D*M:D*M+M].reshape(M, 1)\n",
        "  w = param[D*M+M:].reshape(M+1, 1) # note the +1 for the bias\n",
        "  return v, b, w\n",
        "\n",
        "def error_of_adaptive_model(x, y, param):\n",
        "  v, b, w = extract_parameter(param)\n",
        "  return np.sum(np.square(adaptive_model(x, v, b, w)-y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CiiQZP1OM9N6"
      },
      "source": [
        "<font size=4>We now can calculate the gradient, use the optimizer and visualize the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COro8y-B82xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error_of_adaptive = lambda param: error_of_adaptive_model(x, y, param)\n",
        "grad_objective = grad(error_of_adaptive)\n",
        "result = minimize(error_of_adaptive, numpy.random.rand(M*D+M+M+1), method='CG', jac=grad_objective) # note the +1 for bias\n",
        "print (result.x)\n",
        "\n",
        "xvals = numpy.arange (-3,3,0.1).reshape(-1,1)\n",
        "v, b, w = extract_parameter(result.x)\n",
        "pyplot.figure(figsize=(16,6))\n",
        "pyplot.plot(xvals, numpy.sinc(xvals), c='g', lw=0.5)\n",
        "pyplot.plot(xvals, adaptive_model(xvals.T, v, b, w).T, c='r', lw=0.6)\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlArk9F7HdXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v, b, w = extract_parameter(result.x)\n",
        "pyplot.figure(figsize=(16,6))\n",
        "pyplot.plot(xvals, numpy.sinc(xvals), c='g', lw=0.5, label=\"true\")\n",
        "pyplot.plot(xvals,w[0]*np.ones(xvals.shape), lw=0.5, c='k', label=\"bias\")\n",
        "for i in range(M):\n",
        "  pyplot.plot(xvals, w[i+1]*h(xvals.T, v.T[i].reshape(1,1),b[i].reshape(1,1))[1:].T, lw=0.5, label=\"# \"+str(i))\n",
        "pyplot.plot(xvals, adaptive_model(xvals.T, v, b, w).T, c='r', lw=0.6, label=\"sum\")\n",
        "pyplot.legend()\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Enby9ssbHR",
        "colab_type": "text"
      },
      "source": [
        "<font size=4>\n",
        "\n",
        "As we can see, <font color=#bb0000>$M$</font> defines the number of basis functions.\n",
        "<font size=6><center>\n",
        "\n",
        "[PART B](https://colab.research.google.com/drive/1hIaFKr5At02wUaFHNQpDn0HuhaIE-F6s)"
      ]
    }
  ]
}